{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from datasets.Load_CelebA import CelebAData\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import random\n",
    "import torch\n",
    "def add_noise(input):\n",
    "    noise=torch.randn((3,224,224))/256\n",
    "    return input+noise\n",
    "def paste_random_patch_tensor(image1, image2, patch_size=(100, 100)):\n",
    "    \"\"\"\n",
    "    Paste a random patch from image2 to image1.\n",
    "\n",
    "    :param image1: The image to paste the patch onto. Must be a PyTorch tensor of shape (C, H, W).\n",
    "    :param image2: The image to take the patch from. Must be a PyTorch tensor of shape (C, H, W).\n",
    "    :param patch_size: A tuple specifying the size of the patch (height, width).\n",
    "    :returns: A new tensor which is image1 with the patch from image2 pasted onto it.\n",
    "    \"\"\"\n",
    "    assert image1.shape == image2.shape, \"Images must be the same size\"\n",
    "    _, height, width = image1.shape\n",
    "\n",
    "    top = torch.randint(0, height - patch_size[0], size=(1,)).item()\n",
    "    left = torch.randint(0, width - patch_size[1], size=(1,)).item()\n",
    "\n",
    "    patch = image2[:, top:top+patch_size[0], left:left+patch_size[1]]\n",
    "    image1[:, top:top+patch_size[0], left:left+patch_size[1]] = patch\n",
    "\n",
    "    return image1\n",
    "\n",
    "\n",
    "tranforms_train=transforms.Compose(\n",
    "    [\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        transforms.RandomVerticalFlip(),\n",
    "        transforms.RandomHorizontalFlip()\n",
    "    ]\n",
    ")\n",
    "writer=SummaryWriter(\"log\")\n",
    "\n",
    "root=\"F:/CelebA_Spoof\"\n",
    "data=CelebAData(root_dir=root,train=True,transforms=tranforms_train)\n",
    "image,label=data[0]\n",
    "noisy_image=add_noise(image)\n",
    "\n",
    "image1,label=data[1]\n",
    "\n",
    "cut_image=paste_random_patch_tensor(image1=image,image2=image1)\n",
    "writer.add_image(tag='cut_paste1',img_tensor=cut_image,global_step=0)\n",
    "writer.add_image(tag='cut_paste1',img_tensor=image1,global_step=1)\n",
    "writer.add_image(tag='cut_paste1',img_tensor=image,global_step=2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "writer.close()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 7, 7])\n",
      "14336.0\n"
     ]
    }
   ],
   "source": [
    "from torch\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def cut_paste(self,input1,input2):\n",
    "    width, height = 224, 224\n",
    "\n",
    "    # 裁剪区域的最大宽度和高度\n",
    "    max_patch_width, max_patch_height = 100, 100  # 你可以根据你的需要调整这些值\n",
    "\n",
    "    # 随机生成裁剪区域的位置和大小\n",
    "    x1 = random.randint(0, width - max_patch_width)\n",
    "    y1 = random.randint(0, height - max_patch_height)\n",
    "    w = random.randint(1, width - x1)\n",
    "    h = random.randint(1, height - y1)\n",
    "\n",
    "    # 裁切一块区域\n",
    "    patch = input2[:, y1:y1 + h, x1:x1 + w]\n",
    "\n",
    "    # 随机生成粘贴区域的位置\n",
    "    x2 = random.randint(0, width - w)\n",
    "    y2 = random.randint(0, height - h)\n",
    "\n",
    "    # 将这块区域贴到另外一块区域\n",
    "    input1[:, y2:y2 + h, x2:x2 + w] = patch\n",
    "    return input1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import models\n",
    "from torch import nn\n",
    "model_resnet18=models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "model_resnet50=models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "text_student=nn.Sequential(*(list(models.resnet18(weights=models.ResNet18_Weights.DEFAULT).children())[:-1]))\n",
    "text_student_head=text_student[0:4]\n",
    "print(text_student_head)\n",
    "# print(model_resnet18.layer1)\n",
    "# print(model_resnet18.layer2)\n",
    "# print(model_resnet18.layer3)\n",
    "# print(model_resnet18.layer4)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import models\n",
    "from loss import cosine_similarity_loss\n",
    "from datasets.Load_CelebA import CelebAData\n",
    "from networks.S_T_model import train_difference_extraction_model\n",
    "from torchvision import transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from configs import parse\n",
    "model = train_difference_extraction_model()\n",
    "train_transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "                transforms.RandomVerticalFlip(),\n",
    "                transforms.RandomHorizontalFlip()\n",
    "            ]\n",
    "        )\n",
    "args = parse().parse_args()\n",
    "# writer=SummaryWriter('testlog')\n",
    "# print(args.data_dir)\n",
    "# data=CelebAData(root_dir=\"F:/CelebA_Spoof\",train=True,transforms=train_transform)\n",
    "#\n",
    "# image,label=data[0]\n",
    "#\n",
    "#\n",
    "# image_noise=model.add_noise(image)\n",
    "# image_cut=model.cut_paste(image)\n",
    "#\n",
    "# writer.add_image(\"test\",image,0)\n",
    "# writer.add_image(\"test\",image_noise,1)\n",
    "# writer.add_image(\"test\",image_cut)\n",
    "#\n",
    "# text_normal_feature,text_noise_feature,difference_normal_featue,difference_cutpaste_featue=model(image)\n",
    "#\n",
    "# loss=cosine_similarity_loss(text_normal_feature,text_noise_feature)\n",
    "# print(loss)\n",
    "#\n",
    "# loss=cosine_similarity_loss(difference_normal_featue,difference_cutpaste_featue)\n",
    "#\n",
    "# writer.close()\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from torchvision import models\n",
    "from loss import cosine_similarity_loss\n",
    "from datasets.Load_CelebA import CelebAData\n",
    "from networks.S_T_model import train_difference_extraction_model\n",
    "from torchvision import transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from configs import parse\n",
    "train_transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ]\n",
    "        )\n",
    "args = parse().parse_args()\n",
    "writer=SummaryWriter('testlog')\n",
    "data=CelebAData(root_dir=\"F:/CelebA_Spoof\",train=True,transforms=train_transform,only_true=True)\n",
    "dataloader=DataLoader(data,batch_size=8,shuffle=True)\n",
    "\n",
    "step=0\n",
    "for i in dataloader:\n",
    "    images,labels=i\n",
    "    writer.add_images(\"True\",images,step)\n",
    "    step+=1\n",
    "    if step==20:\n",
    "        break\n",
    "\n",
    "writer.close()\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "root=\"F:/CelebA_Spoof/metas/intra_test\"\n",
    "txt=os.path.join(root,'test_label.txt')\n",
    "image_paths=[]\n",
    "labels=[]\n",
    "with open(txt,'r') as f:\n",
    "    for line in f:\n",
    "        path,label=line.rstrip('\\n').split(' ')\n",
    "        print(f'path:{path}      label:{label}')\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "t='Train_1'\n",
    "if 'Train' in t:\n",
    "    img_use='Train_files'\n",
    "elif 'Test' in t:\n",
    "    img_use='Test_files'\n",
    "elif 'Dev' in t:\n",
    "    img_use='Dev_files'\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "from datasets.Load_CelebA import CelebAData\n",
    "from torchvision import transforms\n",
    "from networks.R50_model import R50\n",
    "from networks.SwinV2_model import SwinV2\n",
    "import torch.optim as optim\n",
    "import argparse\n",
    "from torch.nn import functional as F\n",
    "from configs import parse\n",
    "from utils.performance import test_accuracy\n",
    "from networks.S_T_model import train_difference_extraction_model\n",
    "from  loss import cosine_similarity_loss\n",
    "args = parse().parse_args()\n",
    "train_transform=transforms.Compose(\n",
    "        [\n",
    "            transforms.Normalize(mean=[0.485,0.456,0.406],std=[0.229,0.224,0.225]),\n",
    "            transforms.RandomVerticalFlip(),\n",
    "            transforms.RandomHorizontalFlip()\n",
    "        ]\n",
    "\n",
    "    )\n",
    "test_transform=transforms.Compose(\n",
    "    [\n",
    "        transforms.Normalize(mean=[0.485,0.456,0.406],std=[0.229,0.224,0.225]),\n",
    "        transforms.RandomVerticalFlip(),\n",
    "        transforms.RandomHorizontalFlip()\n",
    "    ]\n",
    ")\n",
    "train_data=CelebAData(root_dir=args.data_dir,train=True,transforms=train_transform)\n",
    "test_data=CelebAData(root_dir=args.data_dir,train=False,transforms=test_transform)\n",
    "train_data_true=CelebAData(root_dir=args.data_dir,train=True,transforms=train_transform,only_true=True)\n",
    "train_loader=DataLoader(train_data,batch_size=args.batch_size,shuffle=True)\n",
    "test_loader=DataLoader(train_data,batch_size=args.batch_size,shuffle=True)\n",
    "train_true_loader=DataLoader(train_data_true,batch_size=args.batch_size,shuffle=True)\n",
    "\n",
    "def cut_paste_single(self, image1,image2, max_patch_size=(100, 100)):\n",
    "    # cut and paste image\n",
    "\n",
    "    assert image1.shape == image2.shape\n",
    "    _, height, width = image1.shape\n",
    "\n",
    "    patch_height = random.randint(1, min(height, max_patch_size[0]) + 1)\n",
    "    patch_width = random.randint(1, min(width, max_patch_size[1]) + 1)\n",
    "\n",
    "    top = random.randint(0, height - patch_height + 1)\n",
    "    left = random.randint(0, width - patch_width + 1)\n",
    "    patch = image2[:, top:top + patch_height, left:left + patch_width]\n",
    "    # 定义一个高斯模糊核\n",
    "    gaussian_kernel = torch.tensor([[1., 2, 1], [2, 4, 2], [1, 2, 1]]) / 16.0\n",
    "    gaussian_kernel = gaussian_kernel.view(1, 1, 3, 3).repeat(3, 1, 1, 1)  # Repeat for each channel\n",
    "\n",
    "    # 对覆盖图像进行高斯模糊\n",
    "    overlay_blurred = F.conv2d(patch.unsqueeze(0), gaussian_kernel, padding=1).squeeze(0)\n",
    "\n",
    "    result = image1.clone()\n",
    "    result[:, top:top + patch_height, left:left + patch_width] = overlay_blurred\n",
    "    return result\n",
    "\n",
    "\n",
    "image1,_=train_data[0]\n",
    "image2,_=train_data[1]\n",
    "writer=SummaryWriter(\"testlog\")\n",
    "\n",
    "writer.add_image(\"cut\",image1,0)\n",
    "result=cut_paste_single(image1,image2)\n",
    "writer.add_image(\"cut\",result,1)\n",
    "writer.add_image(\"cut\",image2,2)\n",
    "writer.close()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import dlib\n",
    "import fnmatch\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import random\n",
    "def process_video( video_path, root_dir, p, t, label, img_use):\n",
    "    detector = dlib.get_frontal_face_detector()\n",
    "    video = cv2.VideoCapture(os.path.join(root_dir, img_use, video_path))\n",
    "    frame_count = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    random_frames = random.sample(range(frame_count), 5)\n",
    "    for frame_number in random_frames:\n",
    "        video.set(1, frame_number)\n",
    "        ret, frame = video.read()\n",
    "        video_name = video_path.split('.')[0]\n",
    "        img_path = os.path.join(root_dir, p, t.split('.')[0], label, f'{video_name}_{frame_number}.png')\n",
    "        if ret:\n",
    "            dets = detector(frame, 1)\n",
    "            if not dets:\n",
    "                print(f'Failed to detector face {p}_{t}_{label}_{video_path}')\n",
    "            for i, d in enumerate(dets):\n",
    "                x1 = d.top() if d.top() > 0 else 0\n",
    "                y1 = d.bottom() if d.bottom() > 0 else 0\n",
    "                x2 = d.left() if d.left() > 0 else 0\n",
    "                y2 = d.right() if d.right() > 0 else 0\n",
    "\n",
    "                if not os.path.exists(os.path.dirname(img_path)):\n",
    "                    try:\n",
    "                        os.makedirs(os.path.dirname(img_path))\n",
    "                    except:\n",
    "                        print('error')\n",
    "\n",
    "                face = frame[x1:y1,x2:y2]\n",
    "                w,h,c=face.shape()\n",
    "                if w<100 and h<100 and c<100:\n",
    "                    print(\"detect fial\")\n",
    "                    continue\n",
    "                cv2.imwrite(img_path, face)\n",
    "        else:\n",
    "            print(f'Failed to read frame {video_name}')\n",
    "    video.release()\n",
    "\n",
    "def process_data(root_dir):\n",
    "    protocol = ['Protocol_1', 'Protocol_2', 'Protocol_3', 'Protocol_4']\n",
    "    for p in protocol:\n",
    "        pro_txt = []\n",
    "        for root, dirs, files in os.walk(os.path.join(root_dir, 'Protocols', p)):\n",
    "            for file in files:\n",
    "                if fnmatch.fnmatch(file, '*.txt'):\n",
    "                    pro_txt.append(file)\n",
    "        for t in pro_txt:\n",
    "            with open(os.path.join(root_dir, 'Protocols', p, t), 'r') as file:\n",
    "                lines = file.readlines()\n",
    "            video_paths = []\n",
    "            labels = []\n",
    "            face_txt_paths = []\n",
    "            for line in lines:\n",
    "                parts = line.strip().split(',')\n",
    "                label = parts[0]\n",
    "                path = parts[1]\n",
    "                video_path = path + '.avi'\n",
    "                video_paths.append(video_path)\n",
    "                labels.append(label)\n",
    "\n",
    "\n",
    "            if 'Train' in t:\n",
    "                img_use = 'Train_files'\n",
    "            elif 'Test' in t:\n",
    "                img_use = 'Test_files'\n",
    "            elif 'Dev' in t:\n",
    "                img_use = 'Dev_files'\n",
    "            with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "                for num in range(len(video_paths)):\n",
    "                    executor.submit(process_video, video_paths[num], root_dir, p, t, labels[num], img_use)\n",
    "\n",
    "\n",
    "\n",
    "root_dir='E:\\\\OULU-NPU'\n",
    "process_data(root_dir)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 128, 56, 56]           6,272\n",
      "         LayerNorm-2          [-1, 56, 56, 128]             256\n",
      "        PatchEmbed-3          [-1, 56, 56, 128]               0\n",
      "          Identity-4          [-1, 56, 56, 128]               0\n",
      "         LayerNorm-5          [-1, 56, 56, 128]             256\n",
      "            Linear-6              [-1, 49, 384]          49,536\n",
      "           Softmax-7            [-1, 4, 49, 49]               0\n",
      "           Dropout-8            [-1, 4, 49, 49]               0\n",
      "            Linear-9              [-1, 49, 128]          16,512\n",
      "          Dropout-10              [-1, 49, 128]               0\n",
      "  WindowAttention-11              [-1, 49, 128]               0\n",
      "         Identity-12          [-1, 56, 56, 128]               0\n",
      "        LayerNorm-13            [-1, 3136, 128]             256\n",
      "           Linear-14            [-1, 3136, 512]          66,048\n",
      "             GELU-15            [-1, 3136, 512]               0\n",
      "          Dropout-16            [-1, 3136, 512]               0\n",
      "           Linear-17            [-1, 3136, 128]          65,664\n",
      "          Dropout-18            [-1, 3136, 128]               0\n",
      "              Mlp-19            [-1, 3136, 128]               0\n",
      "         Identity-20            [-1, 3136, 128]               0\n",
      "SwinTransformerBlock-21          [-1, 56, 56, 128]               0\n",
      "        LayerNorm-22          [-1, 56, 56, 128]             256\n",
      "           Linear-23              [-1, 49, 384]          49,536\n",
      "          Softmax-24            [-1, 4, 49, 49]               0\n",
      "          Dropout-25            [-1, 4, 49, 49]               0\n",
      "           Linear-26              [-1, 49, 128]          16,512\n",
      "          Dropout-27              [-1, 49, 128]               0\n",
      "  WindowAttention-28              [-1, 49, 128]               0\n",
      "         DropPath-29          [-1, 56, 56, 128]               0\n",
      "        LayerNorm-30            [-1, 3136, 128]             256\n",
      "           Linear-31            [-1, 3136, 512]          66,048\n",
      "             GELU-32            [-1, 3136, 512]               0\n",
      "          Dropout-33            [-1, 3136, 512]               0\n",
      "           Linear-34            [-1, 3136, 128]          65,664\n",
      "          Dropout-35            [-1, 3136, 128]               0\n",
      "              Mlp-36            [-1, 3136, 128]               0\n",
      "         DropPath-37            [-1, 3136, 128]               0\n",
      "SwinTransformerBlock-38          [-1, 56, 56, 128]               0\n",
      "SwinTransformerStage-39          [-1, 56, 56, 128]               0\n",
      "        LayerNorm-40          [-1, 28, 28, 512]           1,024\n",
      "           Linear-41          [-1, 28, 28, 256]         131,072\n",
      "     PatchMerging-42          [-1, 28, 28, 256]               0\n",
      "        LayerNorm-43          [-1, 28, 28, 256]             512\n",
      "           Linear-44              [-1, 49, 768]         197,376\n",
      "          Softmax-45            [-1, 8, 49, 49]               0\n",
      "          Dropout-46            [-1, 8, 49, 49]               0\n",
      "           Linear-47              [-1, 49, 256]          65,792\n",
      "          Dropout-48              [-1, 49, 256]               0\n",
      "  WindowAttention-49              [-1, 49, 256]               0\n",
      "         DropPath-50          [-1, 28, 28, 256]               0\n",
      "        LayerNorm-51             [-1, 784, 256]             512\n",
      "           Linear-52            [-1, 784, 1024]         263,168\n",
      "             GELU-53            [-1, 784, 1024]               0\n",
      "          Dropout-54            [-1, 784, 1024]               0\n",
      "           Linear-55             [-1, 784, 256]         262,400\n",
      "          Dropout-56             [-1, 784, 256]               0\n",
      "              Mlp-57             [-1, 784, 256]               0\n",
      "         DropPath-58             [-1, 784, 256]               0\n",
      "SwinTransformerBlock-59          [-1, 28, 28, 256]               0\n",
      "        LayerNorm-60          [-1, 28, 28, 256]             512\n",
      "           Linear-61              [-1, 49, 768]         197,376\n",
      "          Softmax-62            [-1, 8, 49, 49]               0\n",
      "          Dropout-63            [-1, 8, 49, 49]               0\n",
      "           Linear-64              [-1, 49, 256]          65,792\n",
      "          Dropout-65              [-1, 49, 256]               0\n",
      "  WindowAttention-66              [-1, 49, 256]               0\n",
      "         DropPath-67          [-1, 28, 28, 256]               0\n",
      "        LayerNorm-68             [-1, 784, 256]             512\n",
      "           Linear-69            [-1, 784, 1024]         263,168\n",
      "             GELU-70            [-1, 784, 1024]               0\n",
      "          Dropout-71            [-1, 784, 1024]               0\n",
      "           Linear-72             [-1, 784, 256]         262,400\n",
      "          Dropout-73             [-1, 784, 256]               0\n",
      "              Mlp-74             [-1, 784, 256]               0\n",
      "         DropPath-75             [-1, 784, 256]               0\n",
      "SwinTransformerBlock-76          [-1, 28, 28, 256]               0\n",
      "SwinTransformerStage-77          [-1, 28, 28, 256]               0\n",
      "        LayerNorm-78         [-1, 14, 14, 1024]           2,048\n",
      "           Linear-79          [-1, 14, 14, 512]         524,288\n",
      "     PatchMerging-80          [-1, 14, 14, 512]               0\n",
      "        LayerNorm-81          [-1, 14, 14, 512]           1,024\n",
      "           Linear-82             [-1, 49, 1536]         787,968\n",
      "          Softmax-83           [-1, 16, 49, 49]               0\n",
      "          Dropout-84           [-1, 16, 49, 49]               0\n",
      "           Linear-85              [-1, 49, 512]         262,656\n",
      "          Dropout-86              [-1, 49, 512]               0\n",
      "  WindowAttention-87              [-1, 49, 512]               0\n",
      "         DropPath-88          [-1, 14, 14, 512]               0\n",
      "        LayerNorm-89             [-1, 196, 512]           1,024\n",
      "           Linear-90            [-1, 196, 2048]       1,050,624\n",
      "             GELU-91            [-1, 196, 2048]               0\n",
      "          Dropout-92            [-1, 196, 2048]               0\n",
      "           Linear-93             [-1, 196, 512]       1,049,088\n",
      "          Dropout-94             [-1, 196, 512]               0\n",
      "              Mlp-95             [-1, 196, 512]               0\n",
      "         DropPath-96             [-1, 196, 512]               0\n",
      "SwinTransformerBlock-97          [-1, 14, 14, 512]               0\n",
      "        LayerNorm-98          [-1, 14, 14, 512]           1,024\n",
      "           Linear-99             [-1, 49, 1536]         787,968\n",
      "         Softmax-100           [-1, 16, 49, 49]               0\n",
      "         Dropout-101           [-1, 16, 49, 49]               0\n",
      "          Linear-102              [-1, 49, 512]         262,656\n",
      "         Dropout-103              [-1, 49, 512]               0\n",
      " WindowAttention-104              [-1, 49, 512]               0\n",
      "        DropPath-105          [-1, 14, 14, 512]               0\n",
      "       LayerNorm-106             [-1, 196, 512]           1,024\n",
      "          Linear-107            [-1, 196, 2048]       1,050,624\n",
      "            GELU-108            [-1, 196, 2048]               0\n",
      "         Dropout-109            [-1, 196, 2048]               0\n",
      "          Linear-110             [-1, 196, 512]       1,049,088\n",
      "         Dropout-111             [-1, 196, 512]               0\n",
      "             Mlp-112             [-1, 196, 512]               0\n",
      "        DropPath-113             [-1, 196, 512]               0\n",
      "SwinTransformerBlock-114          [-1, 14, 14, 512]               0\n",
      "       LayerNorm-115          [-1, 14, 14, 512]           1,024\n",
      "          Linear-116             [-1, 49, 1536]         787,968\n",
      "         Softmax-117           [-1, 16, 49, 49]               0\n",
      "         Dropout-118           [-1, 16, 49, 49]               0\n",
      "          Linear-119              [-1, 49, 512]         262,656\n",
      "         Dropout-120              [-1, 49, 512]               0\n",
      " WindowAttention-121              [-1, 49, 512]               0\n",
      "        DropPath-122          [-1, 14, 14, 512]               0\n",
      "       LayerNorm-123             [-1, 196, 512]           1,024\n",
      "          Linear-124            [-1, 196, 2048]       1,050,624\n",
      "            GELU-125            [-1, 196, 2048]               0\n",
      "         Dropout-126            [-1, 196, 2048]               0\n",
      "          Linear-127             [-1, 196, 512]       1,049,088\n",
      "         Dropout-128             [-1, 196, 512]               0\n",
      "             Mlp-129             [-1, 196, 512]               0\n",
      "        DropPath-130             [-1, 196, 512]               0\n",
      "SwinTransformerBlock-131          [-1, 14, 14, 512]               0\n",
      "       LayerNorm-132          [-1, 14, 14, 512]           1,024\n",
      "          Linear-133             [-1, 49, 1536]         787,968\n",
      "         Softmax-134           [-1, 16, 49, 49]               0\n",
      "         Dropout-135           [-1, 16, 49, 49]               0\n",
      "          Linear-136              [-1, 49, 512]         262,656\n",
      "         Dropout-137              [-1, 49, 512]               0\n",
      " WindowAttention-138              [-1, 49, 512]               0\n",
      "        DropPath-139          [-1, 14, 14, 512]               0\n",
      "       LayerNorm-140             [-1, 196, 512]           1,024\n",
      "          Linear-141            [-1, 196, 2048]       1,050,624\n",
      "            GELU-142            [-1, 196, 2048]               0\n",
      "         Dropout-143            [-1, 196, 2048]               0\n",
      "          Linear-144             [-1, 196, 512]       1,049,088\n",
      "         Dropout-145             [-1, 196, 512]               0\n",
      "             Mlp-146             [-1, 196, 512]               0\n",
      "        DropPath-147             [-1, 196, 512]               0\n",
      "SwinTransformerBlock-148          [-1, 14, 14, 512]               0\n",
      "       LayerNorm-149          [-1, 14, 14, 512]           1,024\n",
      "          Linear-150             [-1, 49, 1536]         787,968\n",
      "         Softmax-151           [-1, 16, 49, 49]               0\n",
      "         Dropout-152           [-1, 16, 49, 49]               0\n",
      "          Linear-153              [-1, 49, 512]         262,656\n",
      "         Dropout-154              [-1, 49, 512]               0\n",
      " WindowAttention-155              [-1, 49, 512]               0\n",
      "        DropPath-156          [-1, 14, 14, 512]               0\n",
      "       LayerNorm-157             [-1, 196, 512]           1,024\n",
      "          Linear-158            [-1, 196, 2048]       1,050,624\n",
      "            GELU-159            [-1, 196, 2048]               0\n",
      "         Dropout-160            [-1, 196, 2048]               0\n",
      "          Linear-161             [-1, 196, 512]       1,049,088\n",
      "         Dropout-162             [-1, 196, 512]               0\n",
      "             Mlp-163             [-1, 196, 512]               0\n",
      "        DropPath-164             [-1, 196, 512]               0\n",
      "SwinTransformerBlock-165          [-1, 14, 14, 512]               0\n",
      "       LayerNorm-166          [-1, 14, 14, 512]           1,024\n",
      "          Linear-167             [-1, 49, 1536]         787,968\n",
      "         Softmax-168           [-1, 16, 49, 49]               0\n",
      "         Dropout-169           [-1, 16, 49, 49]               0\n",
      "          Linear-170              [-1, 49, 512]         262,656\n",
      "         Dropout-171              [-1, 49, 512]               0\n",
      " WindowAttention-172              [-1, 49, 512]               0\n",
      "        DropPath-173          [-1, 14, 14, 512]               0\n",
      "       LayerNorm-174             [-1, 196, 512]           1,024\n",
      "          Linear-175            [-1, 196, 2048]       1,050,624\n",
      "            GELU-176            [-1, 196, 2048]               0\n",
      "         Dropout-177            [-1, 196, 2048]               0\n",
      "          Linear-178             [-1, 196, 512]       1,049,088\n",
      "         Dropout-179             [-1, 196, 512]               0\n",
      "             Mlp-180             [-1, 196, 512]               0\n",
      "        DropPath-181             [-1, 196, 512]               0\n",
      "SwinTransformerBlock-182          [-1, 14, 14, 512]               0\n",
      "       LayerNorm-183          [-1, 14, 14, 512]           1,024\n",
      "          Linear-184             [-1, 49, 1536]         787,968\n",
      "         Softmax-185           [-1, 16, 49, 49]               0\n",
      "         Dropout-186           [-1, 16, 49, 49]               0\n",
      "          Linear-187              [-1, 49, 512]         262,656\n",
      "         Dropout-188              [-1, 49, 512]               0\n",
      " WindowAttention-189              [-1, 49, 512]               0\n",
      "        DropPath-190          [-1, 14, 14, 512]               0\n",
      "       LayerNorm-191             [-1, 196, 512]           1,024\n",
      "          Linear-192            [-1, 196, 2048]       1,050,624\n",
      "            GELU-193            [-1, 196, 2048]               0\n",
      "         Dropout-194            [-1, 196, 2048]               0\n",
      "          Linear-195             [-1, 196, 512]       1,049,088\n",
      "         Dropout-196             [-1, 196, 512]               0\n",
      "             Mlp-197             [-1, 196, 512]               0\n",
      "        DropPath-198             [-1, 196, 512]               0\n",
      "SwinTransformerBlock-199          [-1, 14, 14, 512]               0\n",
      "       LayerNorm-200          [-1, 14, 14, 512]           1,024\n",
      "          Linear-201             [-1, 49, 1536]         787,968\n",
      "         Softmax-202           [-1, 16, 49, 49]               0\n",
      "         Dropout-203           [-1, 16, 49, 49]               0\n",
      "          Linear-204              [-1, 49, 512]         262,656\n",
      "         Dropout-205              [-1, 49, 512]               0\n",
      " WindowAttention-206              [-1, 49, 512]               0\n",
      "        DropPath-207          [-1, 14, 14, 512]               0\n",
      "       LayerNorm-208             [-1, 196, 512]           1,024\n",
      "          Linear-209            [-1, 196, 2048]       1,050,624\n",
      "            GELU-210            [-1, 196, 2048]               0\n",
      "         Dropout-211            [-1, 196, 2048]               0\n",
      "          Linear-212             [-1, 196, 512]       1,049,088\n",
      "         Dropout-213             [-1, 196, 512]               0\n",
      "             Mlp-214             [-1, 196, 512]               0\n",
      "        DropPath-215             [-1, 196, 512]               0\n",
      "SwinTransformerBlock-216          [-1, 14, 14, 512]               0\n",
      "       LayerNorm-217          [-1, 14, 14, 512]           1,024\n",
      "          Linear-218             [-1, 49, 1536]         787,968\n",
      "         Softmax-219           [-1, 16, 49, 49]               0\n",
      "         Dropout-220           [-1, 16, 49, 49]               0\n",
      "          Linear-221              [-1, 49, 512]         262,656\n",
      "         Dropout-222              [-1, 49, 512]               0\n",
      " WindowAttention-223              [-1, 49, 512]               0\n",
      "        DropPath-224          [-1, 14, 14, 512]               0\n",
      "       LayerNorm-225             [-1, 196, 512]           1,024\n",
      "          Linear-226            [-1, 196, 2048]       1,050,624\n",
      "            GELU-227            [-1, 196, 2048]               0\n",
      "         Dropout-228            [-1, 196, 2048]               0\n",
      "          Linear-229             [-1, 196, 512]       1,049,088\n",
      "         Dropout-230             [-1, 196, 512]               0\n",
      "             Mlp-231             [-1, 196, 512]               0\n",
      "        DropPath-232             [-1, 196, 512]               0\n",
      "SwinTransformerBlock-233          [-1, 14, 14, 512]               0\n",
      "       LayerNorm-234          [-1, 14, 14, 512]           1,024\n",
      "          Linear-235             [-1, 49, 1536]         787,968\n",
      "         Softmax-236           [-1, 16, 49, 49]               0\n",
      "         Dropout-237           [-1, 16, 49, 49]               0\n",
      "          Linear-238              [-1, 49, 512]         262,656\n",
      "         Dropout-239              [-1, 49, 512]               0\n",
      " WindowAttention-240              [-1, 49, 512]               0\n",
      "        DropPath-241          [-1, 14, 14, 512]               0\n",
      "       LayerNorm-242             [-1, 196, 512]           1,024\n",
      "          Linear-243            [-1, 196, 2048]       1,050,624\n",
      "            GELU-244            [-1, 196, 2048]               0\n",
      "         Dropout-245            [-1, 196, 2048]               0\n",
      "          Linear-246             [-1, 196, 512]       1,049,088\n",
      "         Dropout-247             [-1, 196, 512]               0\n",
      "             Mlp-248             [-1, 196, 512]               0\n",
      "        DropPath-249             [-1, 196, 512]               0\n",
      "SwinTransformerBlock-250          [-1, 14, 14, 512]               0\n",
      "       LayerNorm-251          [-1, 14, 14, 512]           1,024\n",
      "          Linear-252             [-1, 49, 1536]         787,968\n",
      "         Softmax-253           [-1, 16, 49, 49]               0\n",
      "         Dropout-254           [-1, 16, 49, 49]               0\n",
      "          Linear-255              [-1, 49, 512]         262,656\n",
      "         Dropout-256              [-1, 49, 512]               0\n",
      " WindowAttention-257              [-1, 49, 512]               0\n",
      "        DropPath-258          [-1, 14, 14, 512]               0\n",
      "       LayerNorm-259             [-1, 196, 512]           1,024\n",
      "          Linear-260            [-1, 196, 2048]       1,050,624\n",
      "            GELU-261            [-1, 196, 2048]               0\n",
      "         Dropout-262            [-1, 196, 2048]               0\n",
      "          Linear-263             [-1, 196, 512]       1,049,088\n",
      "         Dropout-264             [-1, 196, 512]               0\n",
      "             Mlp-265             [-1, 196, 512]               0\n",
      "        DropPath-266             [-1, 196, 512]               0\n",
      "SwinTransformerBlock-267          [-1, 14, 14, 512]               0\n",
      "       LayerNorm-268          [-1, 14, 14, 512]           1,024\n",
      "          Linear-269             [-1, 49, 1536]         787,968\n",
      "         Softmax-270           [-1, 16, 49, 49]               0\n",
      "         Dropout-271           [-1, 16, 49, 49]               0\n",
      "          Linear-272              [-1, 49, 512]         262,656\n",
      "         Dropout-273              [-1, 49, 512]               0\n",
      " WindowAttention-274              [-1, 49, 512]               0\n",
      "        DropPath-275          [-1, 14, 14, 512]               0\n",
      "       LayerNorm-276             [-1, 196, 512]           1,024\n",
      "          Linear-277            [-1, 196, 2048]       1,050,624\n",
      "            GELU-278            [-1, 196, 2048]               0\n",
      "         Dropout-279            [-1, 196, 2048]               0\n",
      "          Linear-280             [-1, 196, 512]       1,049,088\n",
      "         Dropout-281             [-1, 196, 512]               0\n",
      "             Mlp-282             [-1, 196, 512]               0\n",
      "        DropPath-283             [-1, 196, 512]               0\n",
      "SwinTransformerBlock-284          [-1, 14, 14, 512]               0\n",
      "       LayerNorm-285          [-1, 14, 14, 512]           1,024\n",
      "          Linear-286             [-1, 49, 1536]         787,968\n",
      "         Softmax-287           [-1, 16, 49, 49]               0\n",
      "         Dropout-288           [-1, 16, 49, 49]               0\n",
      "          Linear-289              [-1, 49, 512]         262,656\n",
      "         Dropout-290              [-1, 49, 512]               0\n",
      " WindowAttention-291              [-1, 49, 512]               0\n",
      "        DropPath-292          [-1, 14, 14, 512]               0\n",
      "       LayerNorm-293             [-1, 196, 512]           1,024\n",
      "          Linear-294            [-1, 196, 2048]       1,050,624\n",
      "            GELU-295            [-1, 196, 2048]               0\n",
      "         Dropout-296            [-1, 196, 2048]               0\n",
      "          Linear-297             [-1, 196, 512]       1,049,088\n",
      "         Dropout-298             [-1, 196, 512]               0\n",
      "             Mlp-299             [-1, 196, 512]               0\n",
      "        DropPath-300             [-1, 196, 512]               0\n",
      "SwinTransformerBlock-301          [-1, 14, 14, 512]               0\n",
      "       LayerNorm-302          [-1, 14, 14, 512]           1,024\n",
      "          Linear-303             [-1, 49, 1536]         787,968\n",
      "         Softmax-304           [-1, 16, 49, 49]               0\n",
      "         Dropout-305           [-1, 16, 49, 49]               0\n",
      "          Linear-306              [-1, 49, 512]         262,656\n",
      "         Dropout-307              [-1, 49, 512]               0\n",
      " WindowAttention-308              [-1, 49, 512]               0\n",
      "        DropPath-309          [-1, 14, 14, 512]               0\n",
      "       LayerNorm-310             [-1, 196, 512]           1,024\n",
      "          Linear-311            [-1, 196, 2048]       1,050,624\n",
      "            GELU-312            [-1, 196, 2048]               0\n",
      "         Dropout-313            [-1, 196, 2048]               0\n",
      "          Linear-314             [-1, 196, 512]       1,049,088\n",
      "         Dropout-315             [-1, 196, 512]               0\n",
      "             Mlp-316             [-1, 196, 512]               0\n",
      "        DropPath-317             [-1, 196, 512]               0\n",
      "SwinTransformerBlock-318          [-1, 14, 14, 512]               0\n",
      "       LayerNorm-319          [-1, 14, 14, 512]           1,024\n",
      "          Linear-320             [-1, 49, 1536]         787,968\n",
      "         Softmax-321           [-1, 16, 49, 49]               0\n",
      "         Dropout-322           [-1, 16, 49, 49]               0\n",
      "          Linear-323              [-1, 49, 512]         262,656\n",
      "         Dropout-324              [-1, 49, 512]               0\n",
      " WindowAttention-325              [-1, 49, 512]               0\n",
      "        DropPath-326          [-1, 14, 14, 512]               0\n",
      "       LayerNorm-327             [-1, 196, 512]           1,024\n",
      "          Linear-328            [-1, 196, 2048]       1,050,624\n",
      "            GELU-329            [-1, 196, 2048]               0\n",
      "         Dropout-330            [-1, 196, 2048]               0\n",
      "          Linear-331             [-1, 196, 512]       1,049,088\n",
      "         Dropout-332             [-1, 196, 512]               0\n",
      "             Mlp-333             [-1, 196, 512]               0\n",
      "        DropPath-334             [-1, 196, 512]               0\n",
      "SwinTransformerBlock-335          [-1, 14, 14, 512]               0\n",
      "       LayerNorm-336          [-1, 14, 14, 512]           1,024\n",
      "          Linear-337             [-1, 49, 1536]         787,968\n",
      "         Softmax-338           [-1, 16, 49, 49]               0\n",
      "         Dropout-339           [-1, 16, 49, 49]               0\n",
      "          Linear-340              [-1, 49, 512]         262,656\n",
      "         Dropout-341              [-1, 49, 512]               0\n",
      " WindowAttention-342              [-1, 49, 512]               0\n",
      "        DropPath-343          [-1, 14, 14, 512]               0\n",
      "       LayerNorm-344             [-1, 196, 512]           1,024\n",
      "          Linear-345            [-1, 196, 2048]       1,050,624\n",
      "            GELU-346            [-1, 196, 2048]               0\n",
      "         Dropout-347            [-1, 196, 2048]               0\n",
      "          Linear-348             [-1, 196, 512]       1,049,088\n",
      "         Dropout-349             [-1, 196, 512]               0\n",
      "             Mlp-350             [-1, 196, 512]               0\n",
      "        DropPath-351             [-1, 196, 512]               0\n",
      "SwinTransformerBlock-352          [-1, 14, 14, 512]               0\n",
      "       LayerNorm-353          [-1, 14, 14, 512]           1,024\n",
      "          Linear-354             [-1, 49, 1536]         787,968\n",
      "         Softmax-355           [-1, 16, 49, 49]               0\n",
      "         Dropout-356           [-1, 16, 49, 49]               0\n",
      "          Linear-357              [-1, 49, 512]         262,656\n",
      "         Dropout-358              [-1, 49, 512]               0\n",
      " WindowAttention-359              [-1, 49, 512]               0\n",
      "        DropPath-360          [-1, 14, 14, 512]               0\n",
      "       LayerNorm-361             [-1, 196, 512]           1,024\n",
      "          Linear-362            [-1, 196, 2048]       1,050,624\n",
      "            GELU-363            [-1, 196, 2048]               0\n",
      "         Dropout-364            [-1, 196, 2048]               0\n",
      "          Linear-365             [-1, 196, 512]       1,049,088\n",
      "         Dropout-366             [-1, 196, 512]               0\n",
      "             Mlp-367             [-1, 196, 512]               0\n",
      "        DropPath-368             [-1, 196, 512]               0\n",
      "SwinTransformerBlock-369          [-1, 14, 14, 512]               0\n",
      "       LayerNorm-370          [-1, 14, 14, 512]           1,024\n",
      "          Linear-371             [-1, 49, 1536]         787,968\n",
      "         Softmax-372           [-1, 16, 49, 49]               0\n",
      "         Dropout-373           [-1, 16, 49, 49]               0\n",
      "          Linear-374              [-1, 49, 512]         262,656\n",
      "         Dropout-375              [-1, 49, 512]               0\n",
      " WindowAttention-376              [-1, 49, 512]               0\n",
      "        DropPath-377          [-1, 14, 14, 512]               0\n",
      "       LayerNorm-378             [-1, 196, 512]           1,024\n",
      "          Linear-379            [-1, 196, 2048]       1,050,624\n",
      "            GELU-380            [-1, 196, 2048]               0\n",
      "         Dropout-381            [-1, 196, 2048]               0\n",
      "          Linear-382             [-1, 196, 512]       1,049,088\n",
      "         Dropout-383             [-1, 196, 512]               0\n",
      "             Mlp-384             [-1, 196, 512]               0\n",
      "        DropPath-385             [-1, 196, 512]               0\n",
      "SwinTransformerBlock-386          [-1, 14, 14, 512]               0\n",
      "SwinTransformerStage-387          [-1, 14, 14, 512]               0\n",
      "       LayerNorm-388           [-1, 7, 7, 2048]           4,096\n",
      "          Linear-389           [-1, 7, 7, 1024]       2,097,152\n",
      "    PatchMerging-390           [-1, 7, 7, 1024]               0\n",
      "       LayerNorm-391           [-1, 7, 7, 1024]           2,048\n",
      "          Linear-392             [-1, 49, 3072]       3,148,800\n",
      "         Softmax-393           [-1, 32, 49, 49]               0\n",
      "         Dropout-394           [-1, 32, 49, 49]               0\n",
      "          Linear-395             [-1, 49, 1024]       1,049,600\n",
      "         Dropout-396             [-1, 49, 1024]               0\n",
      " WindowAttention-397             [-1, 49, 1024]               0\n",
      "        DropPath-398           [-1, 7, 7, 1024]               0\n",
      "       LayerNorm-399             [-1, 49, 1024]           2,048\n",
      "          Linear-400             [-1, 49, 4096]       4,198,400\n",
      "            GELU-401             [-1, 49, 4096]               0\n",
      "         Dropout-402             [-1, 49, 4096]               0\n",
      "          Linear-403             [-1, 49, 1024]       4,195,328\n",
      "         Dropout-404             [-1, 49, 1024]               0\n",
      "             Mlp-405             [-1, 49, 1024]               0\n",
      "        DropPath-406             [-1, 49, 1024]               0\n",
      "SwinTransformerBlock-407           [-1, 7, 7, 1024]               0\n",
      "       LayerNorm-408           [-1, 7, 7, 1024]           2,048\n",
      "          Linear-409             [-1, 49, 3072]       3,148,800\n",
      "         Softmax-410           [-1, 32, 49, 49]               0\n",
      "         Dropout-411           [-1, 32, 49, 49]               0\n",
      "          Linear-412             [-1, 49, 1024]       1,049,600\n",
      "         Dropout-413             [-1, 49, 1024]               0\n",
      " WindowAttention-414             [-1, 49, 1024]               0\n",
      "        DropPath-415           [-1, 7, 7, 1024]               0\n",
      "       LayerNorm-416             [-1, 49, 1024]           2,048\n",
      "          Linear-417             [-1, 49, 4096]       4,198,400\n",
      "            GELU-418             [-1, 49, 4096]               0\n",
      "         Dropout-419             [-1, 49, 4096]               0\n",
      "          Linear-420             [-1, 49, 1024]       4,195,328\n",
      "         Dropout-421             [-1, 49, 1024]               0\n",
      "             Mlp-422             [-1, 49, 1024]               0\n",
      "        DropPath-423             [-1, 49, 1024]               0\n",
      "SwinTransformerBlock-424           [-1, 7, 7, 1024]               0\n",
      "SwinTransformerStage-425           [-1, 7, 7, 1024]               0\n",
      "       LayerNorm-426           [-1, 7, 7, 1024]           2,048\n",
      "FastAdaptiveAvgPool-427                 [-1, 1024]               0\n",
      "        Identity-428                 [-1, 1024]               0\n",
      "SelectAdaptivePool2d-429                 [-1, 1024]               0\n",
      "         Dropout-430                 [-1, 1024]               0\n",
      "          Linear-431                 [-1, 1000]       1,025,000\n",
      "        Identity-432                 [-1, 1000]               0\n",
      "  ClassifierHead-433                 [-1, 1000]               0\n",
      "================================================================\n",
      "Total params: 87,704,680\n",
      "Trainable params: 87,704,680\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 544.60\n",
      "Params size (MB): 334.57\n",
      "Estimated Total Size (MB): 879.74\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import timm\n",
    "from torchsummary import summary\n",
    "\n",
    "model = timm.create_model('swin_base_patch4_window7_224', pretrained=False)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "# 创建一个随机输入张量，用于获取模型的输入大小\n",
    "input_size = (3, 224, 224)  # 根据模型期望的输入尺寸设置\n",
    "input_tensor = torch.randn(1, *input_size).to(device)\n",
    "\n",
    "# 使用 torchsummary 打印模型信息\n",
    "summary(model, input_size=input_size)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torchvision\n",
    "\n",
    "model=torchvision.models.swin_transformer.SwinTransformer.swin_t"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}